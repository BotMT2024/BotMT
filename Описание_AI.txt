Был написан микросервис для анализа видео с использованием FastAPI

На данный момент в нем реализован функционал принятия видео по API (post запрос)

Как происходит работа сервиса.
После получения видео мы запускает процесс его анализа
Он состоит из двух основных шагов: анализ самого видео и анализ голоса.

На этапе анализа видео мы проверяем:
1. Проверяем, находится ли человек в майке без рукавов. Для этого используется  YOLOv8 из библиотеки ultralytics. Если смогла найти данное нарушение, то мы выдаем рекомендацию о том, что на выступлениях стоит одеваться в более официальный вид.
2. Далее мы проверяем, что человек жестикулировал во время самопрезентации. Для этого мы используем библиотеку mediapipe (solutions.hands). После в случае, если человек не использовал жестикуляцию во время выступления, то выдаем рекомендацию о том, что о том, что жесты помогают спикеру сделать речь выразительной и убедительной
3. После происходит проверка того, что человек держал голову ровно и находился всегда в кадре. Для этого используется алгоритм Виолы-Джонса из библиотеки openCV. В случае, если человек нарушил эти правила, то выдается рекомендация. 
4. И последнее, мы анализируем эмоции человека. А именно проверяем, улыбался ли человек во время выступления. Для этого также используется YOLOv8. Далее выдаем рекомендации в случае если человек выступал с нейтральным выражением лица.

Далее мы получаем голос из видео. Для этого используется библиотека moviepy. После мы переводим голос в текст. Для этого используется библиотека speech_recognition. И уже полученный текст мы анализируем на наличие слов паразитов и выдаем рекомендации, если в речи человека их было много.

После сбора всех необходимых рекомендаций мы отправляет в ответ на запрос json с рекомендациями по полученному видео. 